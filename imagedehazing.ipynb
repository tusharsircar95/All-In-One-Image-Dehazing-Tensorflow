{"cells":[{"metadata":{"id":"XM8S9DjPXjTP","trusted":true},"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.keras.layers import *\n\nimport numpy as np\nimport os, shutil\nimport glob\nimport random\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"i97MpbER7dxY"},"cell_type":"markdown","source":"# **Hyperparameters**"},{"metadata":{"id":"-TVsGGZC7S5F","trusted":true},"cell_type":"code","source":"n_epochs = 10\nbatch_size = 8\nlearning_rate = 1e-4\nweight_decay = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"id":"B6jHdkEX7Bvv"},"cell_type":"markdown","source":"# **Network Definition**"},{"metadata":{"id":"QCEbg0A5X5-b","trusted":true},"cell_type":"code","source":"def haze_net(X):\n  \n  conv1 = Conv2D(3,1,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(X)\n  conv2 = Conv2D(3,3,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(conv1)\n  concat1 = tf.concat([conv1,conv2],axis=-1)\n  \n  conv3 = Conv2D(3,5,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(concat1)\n  concat2 = tf.concat([conv2,conv3],axis=-1)\n  \n  conv4 = Conv2D(3,7,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(concat2)\n  concat3 = tf.concat([conv1,conv2,conv3,conv4],axis=-1)\n  \n  conv5 = Conv2D(3,3,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(concat3)\n  K = conv5\n  \n  output = ReLU(max_value=1.0)(tf.math.multiply(K,X) - K + 1.0)\n  #output = output / 255.0\n  \n  return output","execution_count":null,"outputs":[]},{"metadata":{"id":"DLewmbK37I29"},"cell_type":"markdown","source":"# **Data Loading & Pre-processing**"},{"metadata":{"id":"jIcPe_UCcNn8","trusted":true},"cell_type":"code","source":"def setup_data_paths(orig_images_path,hazy_images_path):\n  \n  orig_image_paths = glob.glob(orig_images_path + \"/*.jpg\")\n  n = len(orig_image_paths) \n  random.shuffle(orig_image_paths)\n  \n  train_keys = orig_image_paths[:int(0.90*n)]\n  val_keys = orig_image_paths[int(0.90*n):]\n  \n  \n  split_dict = {}\n  for key in train_keys:\n    split_dict[key] = 'train'\n  for key in val_keys:\n    split_dict[key] = 'val'\n  \n  train_data = []\n  val_data = []\n  \n  hazy_image_paths = glob.glob(hazy_images_path + \"/*.jpg\")\n  for path in hazy_image_paths:\n    label = path.split('/')[-1]\n    orig_path = orig_images_path + \"/\" + label.split('_')[0] + '_' + label.split('_')[1] + \".jpg\"\n    if(split_dict[orig_path] == 'train'):\n      train_data.append([path,orig_path])\n    else: val_data.append([path,orig_path])\n  \n  return train_data, val_data","execution_count":null,"outputs":[]},{"metadata":{"id":"SKvs6a0jRXXO","trusted":true},"cell_type":"code","source":"def load_image(X):\n  X = tf.io.read_file(X)\n  X = tf.image.decode_jpeg(X,channels=3)\n  X = tf.image.resize(X,(480, 640))\n  X = X / 255.0\n  return X\n\ndef showImage(x):\n  x = np.asarray(x*255,dtype=np.int32)\n  plt.figure()\n  plt.imshow(x)","execution_count":null,"outputs":[]},{"metadata":{"id":"MPV56lybcfne","trusted":true},"cell_type":"code","source":"def create_datasets(train_data,val_data,batch_size):\n  \n  train_ds_hazy = tf.data.Dataset.from_tensor_slices([data[0] for data in train_data]).map(lambda x: load_image(x))\n  train_ds_orig = tf.data.Dataset.from_tensor_slices([data[1] for data in train_data]).map(lambda x: load_image(x))\n  train_ds = tf.data.Dataset.zip((train_ds_hazy,train_ds_orig)).shuffle(100).repeat().batch(batch_size)\n\n  val_ds_hazy = tf.data.Dataset.from_tensor_slices([data[0] for data in val_data]).map(lambda x: load_image(x))\n  val_ds_orig = tf.data.Dataset.from_tensor_slices([data[1] for data in val_data]).map(lambda x: load_image(x))\n  val_ds = tf.data.Dataset.zip((val_ds_hazy,val_ds_orig)).shuffle(100).repeat().batch(batch_size)\n\n  iterator = tf.data.Iterator.from_structure(train_ds.output_types,train_ds.output_shapes)\n  \n  train_init_op = iterator.make_initializer(train_ds)\n  val_init_op = iterator.make_initializer(val_ds)\n  \n  return train_init_op, val_init_op, iterator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(9999)\ntf.reset_default_graph()\ntrain_data, val_data = setup_data_paths(orig_images_path=\"../input/nyu-dehazing/data/original_images\",\n                  hazy_images_path=\"../input/nyu-dehazing/data/training_images\");\ntrain_init_op, val_init_op, iterator = create_datasets(train_data,val_data,batch_size)\nnext_element = iterator.get_next()\n\nX = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\nY = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\ndehazed_X = haze_net(X)\n\nloss = tf.reduce_mean(tf.square(dehazed_X-Y))\noptimizer = tf.train.AdamOptimizer(learning_rate)\ntrainable_variables = tf.trainable_variables()\ngradients_and_vars = optimizer.compute_gradients(loss,trainable_variables)\nclipped_gradients = [(tf.clip_by_norm(gradient,0.1),var) for gradient,var in gradients_and_vars]\noptimizer = optimizer.apply_gradients(gradients_and_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsaver = tf.train.Saver()\nload_path = None\n\nwith tf.device('/gpu:0'):\n  with tf.Session() as sess:\n    #saver.restore(sess,'./models/model_checkpoint_' + str(5) + '.ckpt')\n\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(n_epochs):\n      \n      sess.run(train_init_op)\n      batches = len(train_data) // batch_size\n      epoch_loss = 0.0\n      for batch in range(batches):\n\n        batch_data = sess.run(next_element)\n        #print(batch_data[0].shape,batch_data[1].shape)\n        #print(np.max(batch_data[0]),np.max(batch_data[1]))\n        batch_loss, _ = sess.run([loss,optimizer],feed_dict={X:batch_data[0], Y:batch_data[1]})\n        epoch_loss += batch_loss / float(batches)\n        if batch % 1000 == 0:\n          print(\"Training loss at batch %d: %f\\n\"%(batch,batch_loss))\n            \n      train_loss = epoch_loss\n\n      sess.run(val_init_op)\n      batches= len(val_data) // batch_size\n      epoch_loss = 0.0\n      for batch in range(batches):\n        batch_data = sess.run(next_element)\n        batch_loss = sess.run(loss,feed_dict={X:batch_data[0], Y:batch_data[1]})\n        epoch_loss += batch_loss / float(batches)\n        if batch % 100 == 0:\n          print(\"Validation loss at batch %d: %f\\n\"%(batch,batch_loss))\n          for j in range(-2 + batch_size//2):\n            x = batch_data[0][j].reshape((1,)+batch_data[0][j].shape)\n            y = batch_data[1][j].reshape((1,)+batch_data[1][j].shape)\n            dehazed_x = sess.run(dehazed_X,feed_dict={X:x,Y:y})\n            print(\"Image Number: %d\\n\"%(j))\n            showImage(x[0])\n            showImage(y[0])\n            showImage(dehazed_x[0])\n      val_loss = epoch_loss\n\n      saver.save(sess,'./models/model_checkpoint_' + str(epoch) + '.ckpt')\n      \n      print(\"Epoch %d\\nTraining loss: %f\\nValidation loss: %f\\n\\n\"%(epoch,train_loss,val_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_element = iterator.get_next()\n\nwith tf.Session() as sess:\n  sess.run(val_init_op)\n  \n  for i in range(10):\n    batch_data = sess.run(next_element)\n    for j in range(4):\n      x = batch_data[0][j].reshape((1,)+batch_data[0][j].shape)\n      showImage(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.reset_default_graph()\ntrain_data, val_data = setup_data_paths(orig_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/orig_images\",\n                  hazy_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/hazy_images\");\ntrain_init_op, val_init_op, iterator = create_datasets(train_data,val_data,batch_size)\nnext_element = iterator.get_next()\n\nX = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\nY = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\ndehazed_X = haze_net(X)\n\nloss = tf.reduce_mean(tf.square(dehazed_X-Y))\noptimizer = tf.train.AdamOptimizer(learning_rate)\ntrainable_variables = tf.trainable_variables()\ngradients_and_vars = optimizer.compute_gradients(loss,trainable_variables)\nclipped_gradients = [(tf.clip_by_norm(gradient,0.1),var) for gradient,var in gradients_and_vars]\noptimizer = optimizer.apply_gradients(gradients_and_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saver = tf.train.Saver()\n\ntest_input_folder = \"./All-In-One-Image-Dehazing-Tensorflow/test_images\"\ntest_output_folder = \"./All-In-One-Image-Dehazing-Tensorflow/dehazed_test_images\"\nif not os.path.exists(test_output_folder):\n  os.mkdir(test_output_folder)\n  \nfile_types = ['jpeg','jpg']\n\nwith tf.Session() as sess:\n  saver.restore(sess,'./All-In-One-Image-Dehazing-Tensorflow/models/model_checkpoint_7.ckpt')\n  test_image_paths = []\n  for file_type in file_types:\n    test_image_paths.extend(glob.glob(test_input_folder+\"/*.\"+file_type))\n  \n  \n  for path in test_image_paths:\n    image_label = path.split(test_input_folder)[-1][1:]\n    image = Image.open(path)\n    image = image.resize((640, 480))\n    image = np.asarray(image) / 255.0\n    image = image.reshape((1,) + image.shape)\n    dehazed_image = sess.run(dehazed_X,feed_dict={X:image,Y:image})\n    \n    \n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,10))\n    axes[0].imshow(image[0])\n    axes[1].imshow(dehazed_image[0])\n    fig.tight_layout()\n    \n    dehazed_image = np.asarray(dehazed_image[0] * 255,dtype=np.uint8)\n    mpl.image.imsave(test_output_folder + \"/\" + 'dehazed_' + image_label, dehazed_image)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}